{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVERYTHING TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIG VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH=\"../Datasets/Dataset\"\n",
    "PREPROCESSED_PATH=\"../Preprocessed_Datasets/Dataset\"\n",
    "AUGMENTED_PATH=\"../Augmented_Datasets/Dataset\"\n",
    "AUGMENTED_PATH_TRAIN_EX=\"../Augmented_Datasets/Dataset_train\" # Data exclusively for training, not validating\n",
    "YOLO_PATH=\"../YOLO_Datasets/Dataset\"\n",
    "YOLO_PATH_TRAIN_EX=\"../YOLO_Datasets/Dataset_train\" # Data exclusively for training, not validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(AUGMENTED_PATH):\n",
    "    os.makedirs(AUGMENTED_PATH)\n",
    "if not os.path.exists(AUGMENTED_PATH_TRAIN_EX):\n",
    "    os.makedirs(AUGMENTED_PATH_TRAIN_EX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we will apply the following augmentation techniques:\n",
    "- Hue transformations (-100º to +100º)\n",
    "- Contrast inversion (To simulate dark and light modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_slices(DATASET_PATH, AUGMENTED_PATH, 3, 3, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply resize now and not before because we want all images, including tiles, to keep the image size that the model will take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_dataset_images(DATASET_PATH, AUGMENTED_PATH, 640, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_augmentation(AUGMENTED_PATH, AUGMENTED_PATH_TRAIN_EX, 0.15, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_inversion_augmentation(AUGMENTED_PATH, AUGMENTED_PATH_TRAIN_EX, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the augmented data to the train exclusively folder\n",
    "for file in os.listdir(AUGMENTED_PATH):\n",
    "    shutil.copy(os.path.join(AUGMENTED_PATH, file), AUGMENTED_PATH_TRAIN_EX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORMAT CONVERSION\n",
    "\n",
    "Up to now, we have treated with labelme format datasets, but we need to convert it to YOLOv8 format if we want to train\n",
    "a model, which has the following format:\n",
    "\n",
    "```\n",
    "YOLOv8_Dataset/\n",
    "├── data.yaml\n",
    "├── train/\n",
    "│   ├── images/\n",
    "│   │   ├── img1.jpg\n",
    "│   │   ├── img2.jpg\n",
    "│   │   └── ...\n",
    "│   ├── labels/\n",
    "│   │   ├── img1.txt\n",
    "│   │   ├── img2.txt\n",
    "│   │   └── ...\n",
    "├── valid/\n",
    "│   ├── images/\n",
    "│   │   ├── img1.jpg\n",
    "│   │   ├── img2.jpg\n",
    "│   │   └── ...\n",
    "│   ├── labels/\n",
    "│   │   ├── img1.txt\n",
    "│   │   ├── img2.txt\n",
    "│   │   └── ...\n",
    "└── test/ (OPTIONAL)\n",
    "    ├── images/\n",
    "    │   ├── img1.jpg\n",
    "    │   ├── img2.jpg\n",
    "    │   └── ...\n",
    "    └── labels/\n",
    "        ├── img1.txt\n",
    "        ├── img2.txt\n",
    "        └── ...\n",
    "```\n",
    "\n",
    "The format of the data.yml file is:\n",
    "```\n",
    "path: <path_to_dataset_root_dit>\n",
    "train: <path_to_train_images>\n",
    "val: <path_to_validation_images>\n",
    "test: <path_to_test_images> (OPTIONAL)\n",
    "\n",
    "nc: <number_of_classes>\n",
    "names: ['class1', 'class2', 'class3', ...]\n",
    "```\n",
    "\n",
    "The labels for Instance segmentation have the following format for each annotation:\n",
    "```\n",
    "<class-index> <x1> <y1> <x2> <y2> ... <xn> <yn>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelme_to_yolo(AUGMENTED_PATH_TRAIN_EX, YOLO_PATH_TRAIN_EX, 0.7,[\n",
    "                    \"WebIcon\",\n",
    "                    \"Icon\",\n",
    "                    \"Switch\",\n",
    "                    \"BtnSq\",\n",
    "                    \"BtnPill\",\n",
    "                    \"BtnCirc\",\n",
    "                    \"CheckboxChecked\",\n",
    "                    \"CheckboxUnchecked\",\n",
    "                    \"RadiobtnSelected\",\n",
    "                    \"RadiobtnUnselected\",\n",
    "                    \"TextInput\",\n",
    "                    \"Dropdown\",\n",
    "                    \"Link\",\n",
    "                    \"Text\",\n",
    "                    \"TabActive\",\n",
    "                    \"TabInactive\",\n",
    "                    \"Sidebar\",\n",
    "                    \"Navbar\",\n",
    "                    \"Container\",\n",
    "                    \"Image\",\n",
    "                    \"BrowserURLInput\",\n",
    "                    \"Header\",\n",
    "                    \"Toolbar\",\n",
    "                    \"BrowserToolbar\",\n",
    "                    \"Scrollbar\",\n",
    "                    \"Application\",\n",
    "                    \"Taskbar\",\n",
    "                    \"Dock\",\n",
    "                ], \"seg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform fine-tuning over the mobile-sam model using the hyperparameter tuning provided by Ultralytics to get the\n",
    "best results we can. Since this is a non-standard dataset in terms of object features it is not clear what are the values\n",
    "we should use.\n",
    "\n",
    "We will also configure the training to not do any augmentation over the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# Initialize the YOLO model\n",
    "model = YOLO(\"yolov8s-seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters on dataset for 30 epochs\n",
    "model.train(data=f\"{YOLO_PATH_TRAIN_EX}/data.yaml\", workers=1, epochs=30, optimizer='AdamW', plots=False, save=True, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, fliplr=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"runs/segment/tune2/weights/best.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
